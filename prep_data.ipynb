{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db54c51b6ca5fa",
   "metadata": {},
   "source": [
    "## Prepping Files for Model Juptyer Notebook\n",
    "The purpose of the notebook is to take files in the EdNet-KT1 data set and combine them together while also removing and changing some of the values in the columns. The changes will be outputted to a file called 'combined_dataset.csv'\n",
    "\n",
    "Contributor: Will Sessoms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf023d79a2b5a5d5",
   "metadata": {},
   "source": [
    "#### Data Merging Preparation\n",
    "Our original dataset is comprised of one csv file per each of the 784k students, which results in massive overhead when reading in data. To prevent this, we're taking all of the relevant data and merging it into one unified dataset that can be easily read, navigated, and edited.\n",
    "kt1_dir = \"Data/KT1\"\n",
    "questions_fname = \"Data/contents/questions.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378e25697986153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl # Using polars instead of pandas for speed. >9 million lines in 784k csv files.\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pyarrow as pa # Needed for conversion from polars to pandas\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ee71252848dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load important columns of questions file (finding correct_answer and tags)\n",
    "# This only needs to be done once but we'll reference it multiple times per student interaction\n",
    "questions_fname = \"Data/contents/questions.csv\"\n",
    "kt1_dir  = \"Data/KT1\"\n",
    "\n",
    "questions = (\n",
    "    pl.read_csv(questions_fname)\n",
    "    .with_columns([\n",
    "        pl.col(\"question_id\").str.replace(\"q\", \"\").cast(pl.Int32),\n",
    "        pl.col(\"bundle_id\").str.replace(\"b\", \"\").cast(pl.Int32),\n",
    "        pl.col(\"tags\").cast(pl.Utf8)\n",
    "    ])\n",
    "    .select([\"question_id\", \"correct_answer\", \"bundle_id\", \"tags\"])\n",
    ")\n",
    "\n",
    "\n",
    "student_files = [os.path.join(kt1_dir, f) for f in os.listdir(kt1_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f52b63",
   "metadata": {},
   "source": [
    "### Data Fetching and Merging\n",
    "Here, we take all of the information that we need from each KT1 file and combine it into a single .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each interaction, we take the student_id, question_id, bundle_id, tags, elapsed_time, and whether they answered correctly\n",
    "for file in tqdm(student_files, desc=\"Progress\"):\n",
    "    # Take student_id from filename, remove 'u' prefix to make it int\n",
    "    student_id = int(os.path.basename(file).replace(\"u\", \"\").replace(\".csv\", \"\"))\n",
    "\n",
    "    df = (\n",
    "        pl.read_csv(file)\n",
    "        .with_columns([\n",
    "            pl.lit(student_id).alias(\"student_id\"),\n",
    "            pl.col(\"question_id\").str.replace(\"q\", \"\").cast(pl.Int32), # Remove 'q' prefix to make question_id int\n",
    "        ])\n",
    "        .join(questions, on=\"question_id\", how=\"left\")\n",
    "        .with_columns([\n",
    "            # Adds 'correct' column, which details if student got the question correct\n",
    "            (pl.col(\"user_answer\").str.strip_chars().str.to_lowercase() == pl.col(\"correct_answer\").str.strip_chars().str.to_lowercase())\n",
    "            .cast(pl.Int8)\n",
    "            .alias(\"correct\")\n",
    "        ])\n",
    "        .select([\"student_id\", \"timestamp\", \"question_id\", \"bundle_id\", \"tags\", \"elapsed_time\", \"correct\"])\n",
    "    )\n",
    "\n",
    "    # Tags are currently in a list, we need to flatten them so they work in csv\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"tags\")\n",
    "        .cast(pl.Utf8)\n",
    "        .str.replace_all(r\"\\[|\\]|\\s\", \"\")\n",
    "        .str.replace_all(\",\", \";\")\n",
    "        .alias(\"tags\")\n",
    "    )\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# Sort by student_id, then timestamp\n",
    "final_df = pl.concat(dfs, how=\"vertical\").sort([\"student_id\", \"timestamp\"])\n",
    "\n",
    "fname = \"combined_dataset.csv\"\n",
    "final_df.write_csv(fname)\n",
    "print(f\"Saved {fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7dd57",
   "metadata": {},
   "source": [
    "### Splitting Data\n",
    "\n",
    "Next, we need to split the data into our training and validation sets. Our priority is not to mix students, so none of the student in the training set will be in the validation set and vice versa. We additionally chose to split by student activity level (using the total number of interactions logged) to avoid skewing a set with better/worse students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"combined_dataset.csv\"\n",
    "data = pl.read_csv(dataset)\n",
    "\n",
    "# Make a dataframe that counts the number of interactions per student\n",
    "n_interactions = data.group_by(\"student_id\").len().rename({\"len\": \"n_interactions\"})\n",
    "n_interactions_df = n_interactions.to_pandas()\n",
    "# print(n_interactions.head())\n",
    "\n",
    "# Put into 4 bins based off the number of interations for that student\n",
    "# We do this to make sure that heavily active and inactive students are represented in both training and validation sets\n",
    "bins = pd.qcut(n_interactions_df[\"n_interactions\"], q=4, labels=False, duplicates=\"drop\")\n",
    "n_interactions_df[\"activity_bin\"] = bins\n",
    "\n",
    "# 80/20 split based on the activity bins\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=865)\n",
    "train_idx, val_idx = next(splitter.split(n_interactions_df, n_interactions_df[\"activity_bin\"]))\n",
    "\n",
    "# This splits the students into training and validation sets based on their activity bins to ensure stratified sampling\n",
    "train_students = n_interactions_df.iloc[train_idx][\"student_id\"].tolist()\n",
    "val_students = n_interactions_df.iloc[val_idx][\"student_id\"].tolist()\n",
    "\n",
    "# Sanity check\n",
    "print(f\"Training students: {len(train_students)}\")\n",
    "print(f\"Validation students: {len(val_students)}\")\n",
    "\n",
    "# Filters the original data to create separate dataframes for training and validation students\n",
    "train_df = data.filter(pl.col(\"student_id\").is_in(train_students))\n",
    "val_df = data.filter(pl.col(\"student_id\").is_in(val_students))\n",
    "\n",
    "# Save as parquet file to reduce file size and loading reading speed\n",
    "train_df.write_parquet(\"train_data.parquet\")\n",
    "val_df.write_parquet(\"val_data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
