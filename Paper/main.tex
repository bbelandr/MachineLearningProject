
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{graphicx}

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
EdNet: Predicting Student Success
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Benjamin Belandres, John Cordwell, Daniel Moon, Alex Ogle, Will Sessoms}% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
% \thanks{$^{1}$H. Kwakernaak is with Faculty of Electrical Engineering, Mathematics and Computer Science,
%         University of Twente, 7500 AE Enschede, The Netherlands
%         {\tt\small h.kwakernaak at papercept.net}}%
% \thanks{$^{2}$P. Misra is with the Department of Electrical Engineering, Wright State University,
%         Dayton, OH 45435, USA
%         {\tt\small p.misra at ieee.org}}%
% }






\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}

The purpose of our model is to determine whether a student will get the answer to their next problem correct. The model accomplishes this through features based off a student's answer streak, the question difficulty, the time the student took, etc... The algorithm implemented is a combination of logistic regression, random forest classifier, and Extreme Gradient Boost (XGBoost) Classifier combined by voting classifier ensemble method. Accuracy would be balanced with computational efficiency by taking in only a certain number of students for training the models. Results found the each model to be approximately 71\% validation with each model with some models having more overfitting than others (range from 0\% to 2\% difference depending on model).

\end{abstract}

\section{INTRODUCTION}


Many university students struggle to determine when and how to study most effectively. 
Our project investigates whether machine learning models can predict whether a student 
will answer a question correctly based on their past interactions and behavioral patterns. 
Using the EdNet--KT1 dataset, which contains more than 780{,}000 learner interaction logs 
and detailed question metadata, we developed a pipeline to evaluate what factors contribute 
most to predicting student performance.

Our goals were: (1) to design a reproducible end-to-end learning pipeline, 
(2) to perform feature engineering on temporal and question-level attributes, 
and (3) to evaluate the effectiveness of a simple, interpretable model (Logistic Regression)
before exploring more complex learning approaches. 

We reused the overall data preparation steps from our midterm report, including dataset 
consolidation, cleaning, and the initial baseline models. In this final report, we extend
the analysis with improved feature engineering, thorough model evaluation, and visualization-based 
interpretation of the results.

\section{Dataset}

The data that we used comprised of the KT1 and question data sets from EdNet. The KT1 dataset provided these features: \texttt{user\_answer}, \texttt{question\_id}, \texttt{bundle\_id}, and \texttt{elapsed\_time}. KT1 has 784,309 individual csv files. Each csv file represents an individual student's question history. Over all of the files, students produced answers to a total of 131,441,538 questions that we will use to train and validate our model with. 

A separate file called \texttt{questions.csv} holds all the correct answers to each question. The \texttt{questions.csv} file provided us the features \texttt{question\_id}, \texttt{bundle\_id}, \texttt{explanation\_id}, tags, and the \texttt{correct\_answer}.

Here are some relevant features that get utilized in our project that likely need explanation:

\begin{itemize}
    \item \textbf{bundle\_id}: This feature measures the similarity between questions. Questions sharing the same \texttt{bundle\_id} often share a common passage, picture, or listening material. Additionally, \texttt{bundle\_id}'s that are close in numerical value are also likely to indicate similar questions.
    \item \textbf{tags}: This feature groups similar question concepts. A single question can be associated with multiple tags simultaneously. If two questions share the same tag, they are likely similar in content.
\end{itemize}
% \texttt{explanation\_id} is a similar feature to \texttt{bundle\_id} except that instead of measuring the similarity between the questions, it measures the similarity between the answers. Questions with the same \texttt{explanation\_id} have correct answers for similar reasons.

Most of these data are not remotely ready to be used to train a model on. None of the original features that came from this data set were directly used to make our model. For example, the \texttt{user\_answer} feature from KT1 only tells us if the user answers 'A', 'B', 'C', or 'D'. In order to measure the correctness of the user's answer, we have to compare the user's answer with the \texttt{correct\_answer} feature from the \texttt{questions.csv} file. The structure of this data set leaves much work to do to achieve meaningful results. 

\texttt{elapsed\_time} has quite a large variance, and there seems to be a large gap of values missing in \texttt{bundle\_id} and \texttt{question\_id}. This information needs to be normalized in some way to be used effectively.

Additionally, because the dataset is so large and it spans over hundreds of thousands of files, the overhead for simply reading and modifying the data is extremely computationally expensive. In fact, it was especially expensive when using the popular Python library called Pandas to load and modify the data.

\section{Technical Approach}

Due to the sparse organization and large nature of our dataset, we decided to create an intermediary dataset that combines only the useful features from the original dataset. This allowed us to drastically cut down on loading times, as the vast majority of time spent on processing and feature engineering would be spent on opening over 1 million individual text files rather than processing the data itself.

To do this, we used the library Polars to open and close the files quickly and modify some of the features on-the-fly, by stripping unneeded prefixes from the \texttt{student\_id}, \texttt{question\_id}, and \texttt{bundle\_id} features, fetching the tags related to each question, and converting the student's answer (A, B, C, or D) into a concrete \texttt{correct} value that details if their answer was right or wrong (1 or 0). 

After that, we performed our split on the data, into our intermediary training and validation sets, allowing us to calculate rolling values and averages based on the training set without leaking data to the validation set. In our midterm report, we had issues with data leakage when splitting. To fix this, we split based on the \texttt{student\_id} to ensure that no student was in both sets. To ensure fair splits, we also grouped the students into 4 bins based off of their activity level, and drew students into the training and validation sets from these bins to ensure that active and inactive students are spread equally between the data splits. Finally, we chose to use an 80/20 split. Due to the large size of our dataset, the size of the training set ended up only affecting the model's training time rather than performance. These two splits make up our intermediary dataset, which we used to engineer our features for the final dataset.

Our final version of the dataset incorporates all of our final features that we pass directly to the model. The following details each feature, how we derived it from our dataset, and how we chose to scale it:

\subsection{\texttt{elapsed\_time\_z}}
Our \texttt{elapsed\_time\_z} feature details the z-score of a student's time taken to answer a question relative to their time taken on all questions prior. To avoid temporal leakage, we compute this as a rolling value, smoothed with Bayesian smoothing using \texttt{$\alpha$=20}. This ensures that early interactions will not heavily skew the rolling data.

Originally, we took all of the student's \texttt{elapsed\_time} values and applied standard scaling in the pipeline, but we opted to change this to be scaled to each individual student's performance, since all students will take a different amount of time to answer on average. For example, a student who answers very quickly may have a completely average \texttt{elapsed\_time} value compared to the rest of the students, but relative to them this is actually a long time. Scaling this feature to the individual student allows us to pick up these differences per-student, making this feature much more useful than the original \texttt{elapsed\_time}.

We calculated this feature by iterating through each student's interaction in order of \texttt{timestamp} and simply recalculating their \texttt{elapsed\_time} z-score relative to all of their past attempts. Because this z-score is already scaling the data, we did not scale it in our pipeline

\subsection{\texttt{bayes\_rolling\_acc}}
Our \texttt{bayes\_rolling\_acc} feature is designed to show the current accuracy of the student up to that point, using Bayesian smoothing to account for early entries. To calculate this, we kept a rolling count of each student's accuracy and updated it after each interaction. To reduce extreme values from appearing early in the student's history we implemented Bayesian smoothing with \texttt{$\alpha$=20}, effectively starting us with 20 questions answered assumed at the average accuracy (.653523) across all students combined.

The main purpose of this feature was to profile the students by capturing how well they have been performing so far. This allows the model to tell whether the student is above, below, or plain average without memorizing students, allowing for great generalization on the students in the validation set. We decided not to scale \texttt{bayes\_rolling\_acc} because it is already bounded from 0-1 and scaled to the individual student.

\subsection{\texttt{watched\_lecture}}
Our \texttt{watched\_lecture} feature describes whether the student has watched a lecture relevant to the question in this interaction. We calculated this value by checking through each student's interaction log in the \texttt{KT3} folder, which includes each time the student interacted with a specific lecture. We then took this list for each student, used the \texttt{lectures.csv} file to get the tag associated with each lecture. Finally, we run through that student's question interaction log to check if the student has watched a lecture that matches any of the tags before answering the question, then mark this feature with the corresponding value (1 or 0).

Due to the large number of tags (\textasciitilde200), with each lecture covering only a single tag, this feature is very sparse. Due to this, we didn't expect the feature to provide a large accuracy boost. Instead, it may help us predict better on some of the tricky scenarios where a student with a low accuracy answers a difficult question correctly. This feature does not need scaling because it is essentially a boolean.

\subsection{\texttt{student\_streak}}
Our \texttt{student\_streak} feature simply records the current correct/incorrect streak of the students guesses, with no bound on the time captured. It accounts for both positive and negative streaks with the corresponding positive or negative value, meaning that an incorrect answer changes the streak to -1, rather than 0, starting their incorrect streak. We calculated this by running through each student's interactions in chronological order, keeping track of their current streak until it breaks, then starting a new streak. We decided to use standard scaling on this feature because it's main benefit comes from identifying large streaks, whether positive or negative, rather than needing to preserve the difference between differences in small streaks.

\subsection{\texttt{forgetful\_ben\_score}}
Our \texttt{forgetful\_ben\_score} feature details the accuracy of the model in its last interactions, and is meant to serve as a meaningful short-term feature when streaks are misleading or not useful. We calculated this feature by looking at a rolling window of the 5 most recently answered questions by that student and adding or removing 1 from the total for each question they got right or wrong in that window. We chose this method over simple accuracy tracking so that we avoid have extreme accuracy changes with fewer than 5 previous data points, with this method providing natural smoothing. 

This feature can compliment the downsides of the \texttt{student\_streak} very well, especially if we just ended a streak. For example, if the student got 10 answers correct in a row and just previously missed a question to break the streak, the \texttt{student\_streak} is misleading while the \texttt{forgetful\_ben\_score} will still show a value of 3, which is still very high. In this scenario, we are still able to account for the student's recent streak even though it wasn't recognized by the \texttt{student\_streak} metric. Because there is a small range of possible values for this score, we chose to use minmax scaling to preserve small differences.

\subsection{\texttt{bayes\_question\_acc}}
Our \texttt{bayes\_question\_acc} feature works the same way for questions as our \texttt{bayes\_rolling\_acc} works for students, with a small caveat. Because the questions being answered are part of both datasets, we needed to calculate the rolling accuracy only on the training set answers to avoid leakage, then apply the most recently calculated value to each point in the validation set. To extract this, we sorted each set by \texttt{question\_id} then \texttt{timestamp}, iterated once over the training set to calculate the smoothed accuracy at each point, then finally iterated over the validation set, choosing the most recently calculated \texttt{bayes\_question\_acc} from the training set for each data point. With this strategy, we avoid both leaking data between sets and leaking data from the future. Because this feature is already bounded from 0 to 1 and scaled to the individual question, we did not scale it.

\subsection{\texttt{correct}}
The \texttt{correct} value is our target variable, which details whether the student got the question correct or not with a 1 or a 0. Because our dataset did not explicitly provide this, we had to go to each interaction, check the student's answer, then compare it to the correct answer for that question listed in \texttt{questions.csv} and mark whether the student was correct. We obtained this value as part of our initial dataset aggregation in \texttt{prep\_data.ipynb} so that we could use this column to engineer most of our features that rely on checking if the student was correct on previous questions (streak and accuracy measurements).

\vspace{1em}

\begin{table}[t]
\caption{Algorithms Implemented}
\centering
\begin{tabular}{l}
\hline
\textbf{Algorithm} \\ \hline
Scholastic Gradient Descent Classifier (SGD Classifier) \\
Logistic Regression Model \\
Random Forest Classifier \\
Extreme Gradient Boost Classifier (XGBoost Classifier) \\
Voting Ensemble Classifier \\
\hline
\end{tabular}
\label{tab:algorithms}
\end{table}

\section{Overview of the Algorithm Implementations}

\subsection{Logistic Regression Model}
The logistic regression algorithm (Scikit-Learn library) was implemented due to its computational efficiency with large datasets. The model served as a baseline, achieving 71.40\%. The later inclusion of an SGD classifier was due to logistic regression failing to improve in accuracy with large or nonlinear datasets.

\subsection{Scholastic Gradient Descent Classifier (SGD)}
The SGD classifier improved as dataset size increased, because it updates after each sample instead of after the entire dataset. Accuracy tended to stagnate when using 4000 students, and performance was poor on small datasets ($<1000$).

\subsection{Random Forest Classifier}
The random forest classifier was chosen for its strong ability to learn nonlinear relationships. While effective, its computational cost became prohibitive for very large datasets. The strategy used to mitigate this is discussed in the Model Approach section.

\subsection{Extreme Gradient Boost Classifier (XGBoost)}
The XGBoost classifier provided a balance between strong nonlinear learning performance and computational efficiency—slightly better than random forest. Although not as accurate alone as random forest, it proved extremely valuable in the voting ensemble.

\subsection{Voting Ensemble Classifier}
The voting classifier was implemented to produce a more consistent model across different sample sizes. It provided the most stable accuracy changes as student sample sizes varied. Each constituent model was trained on the same dataset but used different algorithms. SGD received a weight of 1.3 due to having the smallest overfit. Random forest received a lower weight due to its tendency to overfit.

\section{Model Approach}
The goal was to design a model achieving at least 75\% accuracy while remaining computationally efficient. Due to large dataset size, challenges arose when algorithms either failed to capture sufficient patterns or were too computationally expensive (e.g., random forest hyperparameter tuning). To balance accuracy with compute cost, the dataset was sampled, enabling more models—such as random forest and XGBoost—to be trained without crashing. To avoid training bias, the dataset was shuffled upon loading to ensure a diverse sampling of outcomes.

\subsection{Issues of Overfitting}
Training using only 2000 students led to slight overfitting, especially for random forest. This was not surprising given the shuffled dataset, which allowed models to easily learn the dominant patterns but occasionally missed patterns appearing less frequently in the full dataset.

\subsection{Solutions for Overfitting}
To ensure sufficient training data, a logistic regression model was first trained on the entire dataset to establish a reference performance. After testing with 4000-student samples in the training set, a 2000-student training sample was found to provide enough data to avoid overfitting (defined as a training–validation accuracy gap of less than 3\% by our standards). This approach performed well for Logistic Regression, SGD, and XGBoost, but not Random Forest. Hyperparameter tuning was therefore based on F1 score rather than accuracy, as it showed smaller train–validation discrepancies. Validation set size was increased from 200 to 1000, reducing the performance gap by about 5\%. The best models from tuning were then combined using a soft voting classifier—which uses class probabilities rather than majority voting—to produce a more reliable final model.

\section{Results and Discussion}

\subsection{Experimental Setup}

We treated each question attempt as a binary classification example with target
\(\texttt{correct} \in \{0,1\}\).  
As a sanity check we computed a majority baseline by always predicting the most frequent class (``correct'').  
This baseline achieved an overall accuracy of \(\mathbf{65.33\%}\); any useful model must do better than this.

\paragraph{Pipeline 1 (Full Logistic Baseline).}
In the first pipeline we used a relatively small feature set:
elapsed time on the item, \texttt{question\_id}, \texttt{bundle\_id}, and per–question accuracy (the fraction of previous students who answered that question correctly).  
Numeric features were standardized (or MinMax–scaled for \texttt{question\_acc}), and a logistic regression model was trained and evaluated on a large train/validation split of the merged dataset.  
Later, we updated this pipeline with our final dataset including all of our engineered features, and this full logistic model achieved a training accuracy of \(\mathbf{71.40\%}\) and a validation accuracy of \(\mathbf{71.33\%}\), which is already a substantial improvement over the 65\% baseline and shows that even a simple model can still extract meaningful patterns from our data.

\paragraph{Pipeline 2 (Sampled Students, Richer Features).}
The second pipeline, which contains most of the notebook figures was designed to experiment with more sophisticated features and models while keeping computation manageable.  
Instead of training on all \(784{,}000\) students, we randomly sampled 2000 students for training and 1000 students for validation via a custom \texttt{read\_in\_data} function.  
This yielded roughly \(2.6\times 10^5\) interactions but preserved full interaction histories for those students.

On top of the original features, we engineered several new ones:
\begin{itemize}
    \item \textbf{\texttt{bayes\_question\_acc}}: the rolling accuracy of each question based on training data.
    \item \textbf{\texttt{bayes\_rolling\_acc}}: a smoothed rolling estimate of each student’s recent accuracy.
    \item \textbf{\texttt{forgetful\_ben\_score}}: a score designed to capture forgetting or decay effects over time.
    \item \textbf{\texttt{student\_streak}}: the length of the student’s current correct/incorrect streak (positive for correct, negative for incorrect).
    \item \textbf{\texttt{elapsed\_time\_z}}: the rolling z-score for an individual student's time taken to answer.
    \item \textbf{\texttt{watched\_lecture}}: details whether the student has watched a lecture relevant to the current question.
\end{itemize}
We visualized each feature using a boxplot, histogram, and scatterplot (the \texttt{get\_info} function).  
The distributions reveal substantial skew: \texttt{elapsed\_time} has heavy right tails due to long outliers, \texttt{question\_acc} is concentrated in roughly the 55--85\% range, and \texttt{student\_streak} has many short streaks and a few very long positive streaks.  
Based on these plots we standardized \texttt{elapsed\_time} and \texttt{student\_streak}, and applied MinMax scaling to \texttt{bayes\_rolling\_acc}, \texttt{question\_acc}, and \texttt{forgetful\_ben\_score}.  

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{histogram questionacc.png}
    \caption{Distribution of question\_accuracy values across the training dataset. Most questions fall between 0.5 and 0.85 accuracy, indicating moderate difficulty and justifying normalization.}
    \label{fig:question-acc-hist}
\end{figure}


\subsection{Model Comparison on Pipeline 2}

We then compared four individual models and one ensemble, all sharing the same normalized feature set:

\begin{itemize}
    \item Logistic Regression (with tuned penalty and \(C\));
    \item Random Forest Classifier;
    \item XGBoost Classifier;
    \item SGD Classifier (linear model with stochastic optimization); and
    \item a soft VotingClassifier ensemble combining the above four models.
\end{itemize}

Hyperparameters were tuned using GridSearchCV for SGD and logistic regression, and RandomizedSearchCV for XGBoost and Random Forest.  
The best models from these searches were then evaluated on the validation set.  
Table~\ref{tab:accuracy} summarizes the validation accuracies for the sampled–student pipeline, along with the majority and full logistic baselines.

\begin{table}[h]
\centering
\caption{Validation accuracy of different models.  
Pipeline~2 results are based on 2000 training students and 1000 validation students, while the ``Full Logistic'' model is trained on the full train/validation splits.}
\label{tab:accuracy}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Train Acc.} & \textbf{Val Acc.} \\
\midrule
Majority baseline (always ``correct'') & -- & 0.653 \\
Logistic Regression (Pipeline 2)       & 0.706 & 0.715 \\
Random Forest                          & 0.737 & 0.716 \\
XGBoost                                & 0.706 & 0.713 \\
SGD Classifier                         & 0.695 & 0.708 \\
Voting Ensemble                        & 0.716 & 0.716 \\
\midrule
Logistic Regression (Full Pipeline 1)  & 0.714 & \textbf{0.713} \\
\bottomrule
\end{tabular}
\end{table}

Several observations emerge from these results and the associated confusion matrices and classification reports:
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.65\linewidth]{log_reg_confusion_matrix.png}
    \caption{Confusion matrix for the Logistic Regression model.  
    The model achieved a training accuracy of 70.67\% and a validation accuracy of 71.54\%.  
    True positives and true negatives dominate the matrix, indicating that Logistic Regression performs reliably as a baseline classifier on the reduced student-split dataset.}
    \label{fig:logreg_confmat}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{random_forest_confusion_matrix.png}
    \caption{Confusion matrix for the Random Forest Classifier. The model achieved a training accuracy of 73.71\% and a validation accuracy of 71.57\%.}
    \label{fig:rf_confusion_matrix}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{xgboost_confusion_matrix.png}
    \caption{XGBoost Confusion Matrix. Training accuracy = 70.68\%, Validation accuracy = 71.34\%.}
    \label{fig:xgb_confusion}
\end{figure}

\begin{itemize}
    \item All models in Pipeline 2 beat the 65\% majority baseline by roughly 5--6 percentage points on validation, but the gains from extra model complexity are modest.  
    The best ensemble reaches about 71.64\% validation accuracy on the sampled students, which is only slightly better than the standalone logistic model in the same pipeline.
    \item Even though it was trained on a tiny portion of the overall data, the Voting Classifier ensemble performs best overall with 71.64\% validation accuracy).  
    This suggests that using a wide variety of models was effective in sensing many kinds of patterns in the data.
    \item The confusion matrices show that all models are much better at predicting \texttt{correct} (\(y=1\)) than \texttt{incorrect} (\(y=0\)).  
    For example, in the logistic regression results we see recall around 0.85--0.91 for class 1 but only 0.30--0.40 for class 0.  
    In practice, this means the system tends to be optimistic and frequently predicts that students will get questions right.  
    This is not ideal if the main goal is early failure detection.
    \item The ROC curves provide a more nuanced view: AUC scores are roughly in the 0.73--0.75 range, with Random Forest and XGBoost slightly ahead of logistic regression and SGD.  
    The ensemble’s ROC curve lies very close to the best individual curves, indicating that soft voting does not dramatically change the trade–off between true and false positive rates.
    \item The XGBoost cost history (training vs.\ validation log loss over boosting rounds) decreases smoothly without large divergence, suggesting limited overfitting on the sampled students.  
    However, the validation accuracy still saturates below the full logistic model’s performance, reinforcing the idea that data size is a major limiting factor.
\end{itemize}

\subsection{Impact of Feature Engineering}

A key question for us was whether the additional features in Pipeline~2 actually improved predictive power compared to the simpler midterm setup.  
Overall, we found that:

\begin{itemize}
    \item \textbf{\texttt{bayes\_question\_acc}} is the single most valuable engineered feature.  
    When we compared models with and without this feature, we consistently observed a few percentage points of accuracy gain.  
    Intuitively, the average difficulty of a question is a strong global signal.
    \item \textbf{Temporal and student–history features} such as \texttt{bayes\_rolling\_acc}, \texttt{forgetful\_ben\_score}, and \texttt{student\_streak} provide smaller but measurable improvements.  
    The distributions we plotted suggest that these features capture additional structure (e.g., long positive streaks of success), but their effect on validation accuracy is incremental rather than transformative.
    \item Adding more features and more complex models does \emph{not} automatically yield large gains.  
    Despite a substantial amount of engineering and tuning, our best ensemble on the sampled pipeline improved accuracy by only 5.3\% over the baseline and only barely outperformed the simple logistic model trained on more data.
\end{itemize}

In short, the strongest driver of performance in our experiments was the combination of a good difficulty proxy (\texttt{question\_acc}) and lots of data for the logistic model, rather than the additional student–history features or ensemble complexity.  


\section{CONCLUSION}

This project set out to evaluate whether student performance on the EdNet-KT1 dataset could be modeled using simple, interpretable features derived from question difficulty and student interaction behavior. Our ensemble ultimately achieved a training accuracy of 71.64\% and a validation accuracy of 71.64\%, outperforming the majority-class baseline of 65.33\%. Although this gain appears modest, it demonstrates that large datasets and complex models still struggle to predict human rationality.

Several important observations emerged from the modeling process. First, feature engineering mattered far more than model complexity. Early attempts using raw identifiers such as question\_id and bundle\_id added noise without improving accuracy. By contrast, engineered features like bayes\_question\_accuracy, rolling Bayesian accuracy, forgetful\_ben\_score, and student\_streak have captured interpretable behavioral patterns and consistently improved performance. This aligns with our visual analyses, where distribution plots revealed skewed and heavy-tailed features that benefited significantly from normalization.

Second, splitting the dataset by student rather than by row was essential. Row-level random splitting leaked future information from the same student into both sets, inflating scores and preventing generalization. Once we applied student-level stratified splitting, validation accuracy stabilized and produced more realistic estimates of real-world performance. This methodological improvement was one of the most important lessons learned throughout the project.

Third, our logistic regression model provided interpretability benefits that guided future work. The model parameters highlighted which features had the strongest influence on predicting correctness, revealing two insights: (1) \texttt{elapsed\_time} and \texttt{question\_accuracy} were the dominant predictors, and (2) identifiers such as question\_id or bundle\_id were not inherently predictive. This informed our decisions when comparing more complex models, showing that additional features must capture meaningful behavior rather than structural metadata.

Visualizations played an equally important role. Distribution plots exposed skew, outliers, missing values, and the need for normalization. Ablation studies demonstrated the incremental value of each engineered feature, clarifying which ones contributed meaningfully and which should be discarded. Learning curves confirmed that the model generalized well and exhibited neither underfitting nor overfitting. These graphical tools shaped our iterative improvements at every stage.

Finally, while ensemble methods such as Random Forests, SGD Classifiers, and XGBoost were tested, logistic regression remained competitive and more computationally efficient, especially given that Pipeline~2 processed only a subset of roughly 2{,}000 students rather than the full 784{,}000. The ensembles provided valuable comparison points but showed diminishing returns relative to their computational cost.

Overall, this project demonstrates that with carefully engineered features, proper dataset splitting, and a strong emphasis on interpretability, even a relatively simple model can reveal actionable insights about student learning behavior. The pipeline we developed forms a solid foundation for more advanced future work, including skill-based tagging models, sequence models, and deep learning architectures that leverage temporal student patterns more fully.

\section{Code and Design}

Our code is split into several files, each covering a different element of our full pipeline. The following details all relevant files in the order they would be run to create our functioning model:

\subsection{\texttt{prep\_data.ipynb}}

This is the first file of our pipeline. This file takes our original dataset, split across millions of individual files, and extracts the useful features into one combined data file, \texttt{combined\_dataset.csv}. In the latter half, this file splits the combined dataset into our training and validation splits. Each split receives its own parquet file, which acts as our intermediate dataset for creating our final training and validation datasets.

\subsection{\texttt{feature\_eng.ipynb}}

This file is our feature engineering notebook. It takes the data from our split intermediate parquets, and uses it to engineer our final features. Each cell is designed to be run on it's own, extracting on the necessary data, then saving the new feature back to our final dataset files. The final dataset files are also parquet files, and they contain all of the original information, as well as all of our engineered features. This final dataset is designed to be read in needing minimal changes before its ready to be used by the model.

\subsection{\texttt{pipeline1.ipynb}}

This file is our initial model, which we wrote our midterm report about. It uses the entire dataset, which is an excessive amount of data for our purposes. Hence, we made a second pipeline to improve upon this one.

\subsection{\texttt{pipeline2.ipynb}}

This is our second, and final, model pipeline. In this file, we randomly select a subset of the students to reduce the training computation (both memory and computing power) needed, which allowed us to explore many other types of models. For reference, we kept our original full-data logistic regression pipeline from \texttt{pipeline1.ipynb}, so that we could compare it to our new additions that use a largely reduced dataset. With this reduced dataset, we created an ensemble of several different models which ultimately surpassed our original logistic regression model, despite using just a fraction of the data.

\section{Workload Distribution}

Benjamin Belandres played a lead role in the project's inception, taking responsibility for the project idea, finding the dataset, normalization and training, and an amount of the feature engineering.

Daniel Moon handled feature extraction, offered assistance with the \texttt{prep\_data.ipynb} notebook, and conducted the essential data visualization and analysis.

Alex Ogle collaborated closely with Will on feature selection and designing the pipeline1 model. Additionally, Alex designed the pipeline2 ensemble model and contributed detail by developing graphs of the data to support the chosen normalization method.

Will Sessoms focused on feature extraction and was responsible for creating the \texttt{prep\_data.ipynb} notebook used for dataset consolidation. Will largely contributed to the feature engineering, and worked with Alex on the pipeline1 file.

John Cordwell provided key guidance in the early stages, focusing on feature selection and the crucial preprocessing planning. Unfortunately, John dropped the class halfway through the semester, but his presence is missed and his contributions are felt.





\newpage
\thispagestyle{empty}
\null

\newpage
\thispagestyle{empty}
\null
\newpage
\thispagestyle{empty}
\null

\newpage
\thispagestyle{empty}
\null




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

This electronic document is a ``live'' template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

This template, modified in MS Word 2003 and saved as ``Word 97-2003 \& 6.0/95 -- RTF'' for the PC, provides authors with most of the formatting specifications needed for preparing electronic versions of their papers. All standard paper components have been specified for three reasons: (1) ease of use when formatting individual papers, (2) automatic compliance to electronic requirements that facilitate the concurrent or later production of electronic products, and (3) conformity of style throughout a conference proceedings. Margins, column widths, line spacing, and type styles are built-in; examples of the type styles are provided throughout this document and are identified in italic type, within parentheses, following the example. Some components, such as multi-leveled equations, graphics, and tables are not prescribed, although the various table text styles are provided. The formatter will need to create these components, incorporating the applicable criteria that follow.

\section{PROCEDURE FOR PAPER SUBMISSION}

\subsection{Selecting a Template (Heading 2)}

First, confirm that you have the correct template for your paper size. This template has been tailored for output on the US-letter paper size. Please do not use it for A4 paper since the margin requirements for A4 papers may be different from Letter paper size.

\subsection{Maintaining the Integrity of the Specifications}

The template is used to format your paper and style the text. All margins, column widths, line spaces, and text fonts are prescribed; please do not alter them. You may note peculiarities. For example, the head margin in this template measures proportionately more than is customary. This measurement and others are deliberate, using specifications that anticipate your paper as one part of the entire proceedings, and not as an independent document. Please do not revise any of the current designations

\section{MATH}

Before you begin to format your paper, first write and save the content as a separate text file. Keep your text and graphic files separate until after the text has been formatted and styled. Do not use hard tabs, and limit use of hard returns to only one return at the end of a paragraph. Do not add any kind of pagination anywhere in the paper. Do not number text heads-the template will do that for you.

Finally, complete content and organizational editing before formatting. Please take note of the following items when proofreading spelling and grammar:

\subsection{Abbreviations and Acronyms} Define abbreviations and acronyms the first time they are used in the text, even after they have been defined in the abstract. Abbreviations such as IEEE, SI, MKS, CGS, sc, dc, and rms do not have to be defined. Do not use abbreviations in the title or heads unless they are unavoidable.

\subsection{Units}

\begin{itemize}

\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
\item Do not mix complete spellings and abbreviations of units: ``Wb/m2'' or ``webers per square meter'', not ``webers/m2''.  Spell out units when they appear in text: ``\ldots a few henries'', not ``\ldots a few H''.
\item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm$^3$'', not ``cc''. (bullet list)

\end{itemize}


\subsection{Equations}

The equations are an exception to the prescribed specifications of this template. You will need to determine whether or not your equation should be typed using either the Times New Roman or the Symbol font (please no other font). To create multileveled equations, it may be necessary to treat the equation as a graphic and insert it into the text after your paper is styled. Number equations consecutively. Equation numbers, within parentheses, are to position flush right, as in (1), using a right tab stop. To make your equations more compact, you may use the solidus ( / ), the exp function, or appropriate exponents. Italicize Roman symbols for quantities and variables, but not Greek symbols. Use a long dash rather than a hyphen for a minus sign. Punctuate equations with commas or periods when they are part of a sentence, as in
\begin{equation}
\alpha + \beta = \chi
\end{equation}

Note that the equation is centered using a center tab stop. Be sure that the symbols in your equation have been defined before or immediately following the equation. Use ``(1)'', not ``Eq. (1)'' or ``equation (1)'', except at the beginning of a sentence: ``Equation (1) is\ldots''

\subsection{Some Common Mistakes}
\begin{itemize}


\item The word ``data'' is plural, not singular.
\item The subscript for the permeability of vacuum ?0, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
\item In American English, commas, semi-/colons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
\item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
\item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
\item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
\item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
\item Do not confuse ``imply'' and ``infer''.
\item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
\item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
\item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.

\end{itemize}


\section{USING THE TEMPLATE}

Use this sample document as your LaTeX source file to create your document. Save this file as {\bf root.tex}. You have to make sure to use the cls file that came with this distribution. If you use a different style file, you cannot expect to get required margins. Note also that when you are creating your out PDF file, the source file is only part of the equation. \emph{Your \TeX\ $\rightarrow$ PDF filter determines the output file size. Even if you make all the specifications to output a letter file in the source - if you filter is set to produce A4, you will only get A4 output.}

It is impossible to account for all possible situation, one would encounter using \TeX. If you are using multiple \TeX\ files you must make sure that the ``MAIN`` source file is called root.tex - this is particularly important if your conference is using PaperPlaza's built in \TeX\ to PDF conversion tool.

\subsection{Headings, etc}

Text heads organize the topics on a relational, hierarchical basis. For example, the paper title is the primary text head because all subsequent material relates and elaborates on this one topic. If there are two or more sub-topics, the next level head (uppercase Roman numerals) should be used and, conversely, if there are not at least two sub-topics, then no subheads should be introduced. Styles named ``Heading 1'', ``Heading 2'', ``Heading 3'', and ``Heading 4'' are prescribed.

\subsection{Figures and Tables}

Positioning Figures and Tables: Place figures and tables at the top and bottom of columns. Avoid placing them in the middle of columns. Large figures and tables may span across both columns. Figure captions should be below the figures; table heads should appear above the tables. Insert figures and tables after they are cited in the text. Use the abbreviation ``Fig. 1'', even at the beginning of a sentence.

\begin{table}[h]
\caption{An Example of a Table}
\label{table_example}
\begin{center}
\begin{tabular}{|c||c|}
\hline
One & Two\\
\hline
Three & Four\\
\hline
\end{tabular}
\end{center}
\end{table}


   \begin{figure}[thpb]
      \centering
      \framebox{\parbox{3in}{We suggest that you use a text box to insert a graphic (which is ideally a 300 dpi TIFF or EPS file, with all fonts embedded) because, in an document, this method is somewhat more stable than directly inserting a picture.
}}
      %\includegraphics[scale=1.0]{figurefile}
      \caption{Inductance of oscillation winding on amorphous
       magnetic core versus DC bias magnetic field}
      \label{figurelabel}
   \end{figure}
   

Figure Labels: Use 8 point Times New Roman for Figure labels. Use words rather than symbols or abbreviations when writing Figure axis labels to avoid confusing the reader. As an example, write the quantity ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including units in the label, present them within parentheses. Do not label axes only with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization {A[m(1)]}'', not just ``A/m''. Do not label axes with a ratio of quantities and units. For example, write ``Temperature (K)'', not ``Temperature/K.''

\section{CONCLUSIONS}

A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}

Appendixes should appear before the acknowledgment.

\section*{ACKNOWLEDGMENT}

The preferred spelling of the word ``acknowledgment'' in America is without an ``e'' after the ``g''. Avoid the stilted expression, ``One of us (R. B. G.) thanks . . .''  Instead, try ``R. B. G. thanks''. Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.



\begin{thebibliography}{99}

\bibitem{c1} G. O. Young, ``Synthetic structure of industrial plastics (Book style with paper title and editor),'' 	in Plastics, 2nd ed. vol. 3, J. Peters, Ed.  New York: McGraw-Hill, 1964, pp. 15--64.
\bibitem{c2} W.-K. Chen, Linear Networks and Systems (Book style).	Belmont, CA: Wadsworth, 1993, pp. 123--135.
\bibitem{c3} H. Poor, An Introduction to Signal Detection and Estimation.   New York: Springer-Verlag, 1985, ch. 4.
\bibitem{c4} B. Smith, ``An approach to graphs of linear forms (Unpublished work style),'' unpublished.
\bibitem{c5} E. H. Miller, ``A note on reflector arrays (Periodical styleÑAccepted for publication),'' IEEE Trans. Antennas Propagat., to be publised.
\bibitem{c6} J. Wang, ``Fundamentals of erbium-doped fiber amplifiers arrays (Periodical styleÑSubmitted for publication),'' IEEE J. Quantum Electron., submitted for publication.
\bibitem{c7} C. J. Kaufman, Rocky Mountain Research Lab., Boulder, CO, private communication, May 1995.
\bibitem{c8} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interfaces(Translation Journals style),'' IEEE Transl. J. Magn.Jpn., vol. 2, Aug. 1987, pp. 740--741 [Dig. 9th Annu. Conf. Magnetics Japan, 1982, p. 301].
\bibitem{c9} M. Young, The Techincal Writers Handbook.  Mill Valley, CA: University Science, 1989.
\bibitem{c10} J. U. Duncombe, ``Infrared navigationÑPart I: An assessment of feasibility (Periodical style),'' IEEE Trans. Electron Devices, vol. ED-11, pp. 34--39, Jan. 1959.
\bibitem{c11} S. Chen, B. Mulgrew, and P. M. Grant, ``A clustering technique for digital communications channel equalization using radial basis function networks,'' IEEE Trans. Neural Networks, vol. 4, pp. 570--578, July 1993.
\bibitem{c12} R. W. Lucky, ``Automatic equalization for digital communication,'' Bell Syst. Tech. J., vol. 44, no. 4, pp. 547--588, Apr. 1965.
\bibitem{c13} S. P. Bingulac, ``On the compatibility of adaptive controllers (Published Conference Proceedings style),'' in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 8--16.
\bibitem{c14} G. R. Faulhaber, ``Design of service systems with priority reservation,'' in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 3--8.
\bibitem{c15} W. D. Doyle, ``Magnetization reversal in films with biaxial anisotropy,'' in 1987 Proc. INTERMAG Conf., pp. 2.2-1--2.2-6.
\bibitem{c16} G. W. Juette and L. E. Zeffanella, ``Radio noise currents n short sections on bundle conductors (Presented Conference Paper style),'' presented at the IEEE Summer power Meeting, Dallas, TX, June 22--27, 1990, Paper 90 SM 690-0 PWRS.
\bibitem{c17} J. G. Kreifeldt, ``An analysis of surface-detected EMG as an amplitude-modulated noise,'' presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
\bibitem{c18} J. Williams, ``Narrow-band analyzer (Thesis or Dissertation style),'' Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
\bibitem{c19} N. Kawasaki, ``Parametric study of thermal and chemical nonequilibrium nozzle flow,'' M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
\bibitem{c20} J. P. Wilkinson, ``Nonlinear resonant circuit devices (Patent style),'' U.S. Patent 3 624 12, July 16, 1990. 






\end{thebibliography}




\end{document}